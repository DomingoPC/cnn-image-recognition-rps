{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc9e79e-4677-4305-aef3-47cb77b03207",
   "metadata": {},
   "source": [
    "# Modelo de clasificación de imágenes: piedra, papel, tijeras.\n",
    "Trabajo de la asignatura de \"Deep Learning\" del Máster en Data Science y Big Data de la Universidad de Sevilla.\n",
    "\n",
    "## Sobre las referencias del trabajo\n",
    "Las referencias de todos los archivos del trabajo están especificadas en el pdf adjunto.\n",
    "\n",
    "Por ejemplo, se podrá encontrar \n",
    "\n",
    "        [C] https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras\n",
    "\n",
    "que sigue la misma numeración que la indicada en las referencias del pdf (C, en este caso) y, además, se indica la url para facilitar el acceso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b844340-300f-481e-92d5-41c6637da2be",
   "metadata": {},
   "source": [
    "## Importación de librerías <a class=\"anchor\" id=\"librerias\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb190fa-9195-48dd-9dcf-6767292f06fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os \n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# Importamos archivos .py auxiliares\n",
    "import redes_neuronales as m # modelos con los que vamos a trabajar \n",
    "import funciones_estudio_modelo as f # funciones últiles para la visualización e interpretación de resultados\n",
    "\n",
    "# Aumento de datos: clase Sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Carga de datos\n",
    "from sklearn.model_selection import train_test_split # partición train, validation, test\n",
    "\n",
    "# Visualización del modelo\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Medidas de rendimiento y visualizaciones\n",
    "from livelossplot import PlotLossesKerasTF # Plot de la función loss en cada época\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as skm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272c3e5-c6cc-41ff-b877-d819ea254d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcab67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semilla aleatoria\n",
    "# [C] https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras\n",
    "\n",
    "seed=33\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee03d678-8ebd-402b-a6bb-a2a6634d6d2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Carga de datos mediante la clase Sequence <a class=\"anchor\" id=\"datos-sequence\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d545ac-3e53-4060-96f9-be413f2d8df6",
   "metadata": {},
   "source": [
    "### Clase Sequence <a class=\"anchor\" id=\"clase-sequence\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab88f5a-793c-4f92-bc77-303edebd1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referencias para la estructura de la clase Sequence:\n",
    "# * Práctica 8 de la asignatura\n",
    "# * [1] https://stackoverflow.com/questions/70230687/how-keras-utils-sequence-works\n",
    "# * ChatGPT: ejemplo básico de funcionamiento añadiendo aumento de datos\n",
    "\n",
    "\n",
    "class RockPaperScissorsSequence(tf.keras.utils.Sequence):\n",
    "  # --- Atributos obligatorios de la clase ---\n",
    "  # Inicialización\n",
    "  def __init__(self, image_paths, labels, batch_size, image_size, shuffle=True, augment=False):\n",
    "      self.image_paths = np.array(image_paths) # ruta de las imágenes\n",
    "      self.labels = labels # etiquetas de las imágenes ('rock', 'paper' o 'scissors')\n",
    "\n",
    "      # Corrección de la IA de Google Colab:\n",
    "      # Como usamos model.compile(..., loss='sparse_categorical_crossentropy), las etiquetas deben ser numéricas. Vamos a mapearlo:\n",
    "      self.class_mapping = {'rock': 0, 'paper': 1, 'scissors': 2} # Map string labels to numerical values\n",
    "\n",
    "      self.batch_size = batch_size # número de imágenes en cada batch\n",
    "      self.image_size = image_size # tamaño de imagen que se le da a la red\n",
    "      self.data_length = len(self.image_paths) # número de imágenes\n",
    "\n",
    "      self.shuffle = shuffle # mezclar imágenes (al final de cada época)\n",
    "      # Preparamos índices y mezclamos si es necesario\n",
    "      self.indexes = np.arange(len(self.image_paths))\n",
    "      if self.shuffle:\n",
    "          np.random.shuffle(self.indexes)\n",
    "\n",
    "      self.augment = augment # modificar imágenes (reducción sobreajuste)\n",
    "      # Parámetros para aumento de datos si es necesario\n",
    "      if self.augment:\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rotation_range=90,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "\n",
    "            shear_range=0.2,\n",
    "            fill_mode='nearest',\n",
    "            brightness_range=[0.8, 1.2]\n",
    "        )\n",
    "      else:\n",
    "        self.datagen = None\n",
    " \n",
    "\n",
    "  # Selección y carga de imágenes para cada batch\n",
    "  def __getitem__(self, index):\n",
    "    # Selecciona índices del batch a partir de los índices mezclados\n",
    "    batch_indexes = self.indexes[\n",
    "        (index * self.batch_size) : ((index + 1) * self.batch_size)\n",
    "    ]\n",
    "\n",
    "    # Rutas de las imágenes del batch\n",
    "    # [2] https://stackoverflow.com/questions/50997928/typeerror-only-integer-scalar-arrays-can-be-converted-to-a-scalar-index-with-1d\n",
    "    batch_paths = self.image_paths[batch_indexes.astype(int)]\n",
    "\n",
    "    # A partir de las rutas, extraemos las categorías ('rock', 'paper' o 'scissors')\n",
    "    batch_labels = [self.labels[path] for path in batch_paths]\n",
    "\n",
    "    # Cargamos las imágenes\n",
    "    X, y = self.__data_generation(batch_paths, batch_labels)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "  # Número de batches por época\n",
    "  def __len__(self):\n",
    "      return int(self.data_length // self.batch_size)\n",
    "\n",
    "\n",
    "  # --- Atributos opcional: actualización cada tras época ---\n",
    "  def on_epoch_end(self):\n",
    "      # Actualizar índices tras cada época\n",
    "      self.indexes = np.arange(self.data_length)\n",
    "      if self.shuffle:\n",
    "          np.random.shuffle(self.indexes)\n",
    "\n",
    "\n",
    "  # --- Otros atributos ---\n",
    "  def __data_generation(self, batch_paths, batch_labels):\n",
    "      # Inicializamos salidas\n",
    "      X = np.empty((self.batch_size, *self.image_size, 3))\n",
    "      y = np.empty((self.batch_size), dtype=int) # debe ser int (hay que mapear clases)\n",
    "\n",
    "      for i, (path, label) in enumerate(zip(batch_paths, batch_labels)):\n",
    "          # Cargamos la imagen en path\n",
    "          img = cv2.imread(path)\n",
    "          img = cv2.resize(img, self.image_size) # Preprocesado: re-escalado de la imagen\n",
    "\n",
    "          # Aumento de datos\n",
    "          # [3] https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "          if self.augment:\n",
    "              # Generar transformación aleatoria con los parámetros fijados en __init__\n",
    "              img = self.datagen.random_transform(x=img)\n",
    "\n",
    "          X[i,] = img / 255.0\n",
    "          y[i] = self.class_mapping[label] # Mapeo de la clase de la imagen (continuación de corrección de Colab AI)\n",
    "\n",
    "      return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f74e30-59e5-4fb3-82c2-06300c490b92",
   "metadata": {},
   "source": [
    "### Carga de datos <a class=\"anchor\" id=\"carga-de-datos\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd5c1c-9a0d-49fa-a922-f1714ce53d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preparamos los datos ---\n",
    "input_path = 'archive' # ruta en local\n",
    "labels = {}\n",
    "image_paths = []\n",
    "\n",
    "# Create a dictionary mapping image paths to their labels\n",
    "for clase in ['rock', 'paper', 'scissors']:\n",
    "    directorio_clase = os.path.join(input_path, clase) # dirección de carpeta con fotos\n",
    "\n",
    "    # Recorremos lista de imagenes en la carpeta\n",
    "    for fname in os.listdir(directorio_clase):\n",
    "      # Diccionario con key = ruta y value = clase\n",
    "        ruta = os.path.join(directorio_clase, fname)\n",
    "        image_paths.append(ruta)\n",
    "        labels[ruta] = clase\n",
    "\n",
    "# Separamos en train / test\n",
    "train, test_val = train_test_split(image_paths, test_size=0.2, random_state=33)\n",
    "validation, test = train_test_split(test_val, test_size=0.7, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b6a60-2169-4bcb-98e1-3d08693d7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cargamos la clase Sequence para cada conjunto de datos ---\n",
    "# Parámetros para cargar las imágenes (clase Sequence)\n",
    "batch_size = 8 # Tamaño de cada batch (8 imágenes simultáneamente)\n",
    "image_size = (128,128)  # Tamaño de las imágenes que le damos al modelo\n",
    "\n",
    "# Objetos para secuenciar las imágenes de cada partición de los datos\n",
    "train_seq = RockPaperScissorsSequence(image_paths=train,\n",
    "                                      labels=labels,\n",
    "                                      batch_size=batch_size,\n",
    "                                      image_size=image_size,\n",
    "                                      augment=True, # aumentamos los datos para dar generalidad al modelo\n",
    "                                      shuffle=True)\n",
    "\n",
    "validation_seq = RockPaperScissorsSequence(image_paths=validation,\n",
    "                                           labels=labels,\n",
    "                                           batch_size=batch_size,\n",
    "                                           image_size=image_size,\n",
    "                                           augment=False, # no modificamos las imágenes de validación\n",
    "                                           shuffle=True) # hay que mezclar, se reutilizan en cada época\n",
    "\n",
    "test_seq = RockPaperScissorsSequence(image_paths=test,\n",
    "                                     labels=labels,\n",
    "                                     batch_size=1,\n",
    "                                     image_size=image_size,\n",
    "                                     augment=False, # no modificamos las imágenes de test\n",
    "                                     shuffle=False) # no es necesario mezclar, porque no se reutilizan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a06206-9eba-4b9f-b666-b5d37aae8a29",
   "metadata": {},
   "source": [
    "## Construcción y entrenamiento de las redes neuronales propuestas <a class=\"anchor\" id=\"red-neuronal\"></a>\n",
    "\n",
    "Tenemos varias técnicas para construir redes neuronales que predigan con precisión los datos. La primera que vamos a ver en este trabajo es el \"Transfer Learning\", que consiste en cargar los pesos de una red neuronal, entrenada con otro conjunto de datos. A esta, le quitaremos las capas superiores (las cercanas a la salida) para, a continuación, añadir las que requiere nuestro conjunto de datos, que al menos será una capa densa de 3 neuronas.\n",
    "\n",
    "Por otro lado, hemos construido un modelo siguiendo las pautas de [5], que podemos resumir en que debemos empezar por el modelo más simple posible y, a partir de él, construir uno más complejo si nuestro sistema lo necesita.\n",
    "\n",
    "En cada caso, veremos dos aproximaciones al problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c25ec0-047d-42d9-88c4-152c15612416",
   "metadata": {},
   "source": [
    "### Transfer Learning <a class=\"anchor\" id=\"transfer-learning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ccf7e-209d-4db5-84a7-c49b7ac9245f",
   "metadata": {},
   "source": [
    "#### VGG16 <a class=\"anchor\" id=\"vgg16\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33ec26-1fad-451a-9bc9-4fab092e9eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(\n",
    "    tf.keras.applications.VGG16(weights=None, include_top=False, input_shape=(*image_size, 3)),\n",
    "    to_file=\"images/VGG16.jpeg\"\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa89d3-4afb-4272-9011-9b2536e487d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelVGG16 = m.modeloVGG16(image_size=image_size)\n",
    "\n",
    "modelVGG16.summary()\n",
    "f.memoria_modelo(np.sum(\n",
    "    [np.prod(v.get_shape().as_list()) for v in modelVGG16.trainable_variables]\n",
    "), tipo='entrenables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a5bd2-f9a2-43bc-ae1c-cc537a7121ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preparamos el entrenamiento del modelo ---\n",
    "# [6] https://stackoverflow.com/questions/58565394/what-is-the-difference-between-sparse-categorical-crossentropy-and-categorical-c\n",
    "modelVGG16.compile(optimizer=tf.keras.optimizers.Adam(1e-3), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Detener el entrenamiento si no hay mejoras significativas (loss)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# --- Entrenamiento ----\n",
    "# Train the model\n",
    "modelVGG16.fit(train_seq, validation_data=validation_seq, # datos de entrenamiento y validación\n",
    "          epochs=10,\n",
    "          callbacks = [early_stopping, PlotLossesKerasTF()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5402459-7aca-4cf9-94a9-18109aedf5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Fine Tuning ---\n",
    "# Liberamos algunas capas para hacer fine tuning\n",
    "capas_vgg_descongeladas = 5\n",
    "for layer in modelVGG16.get_layer('vgg16').layers[-capas_vgg_descongeladas:]:\n",
    "    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "# Preparamos el entrenamiento del modelo\n",
    "# [6] https://stackoverflow.com/questions/58565394/what-is-the-difference-between-sparse-categorical-crossentropy-and-categorical-c\n",
    "modelVGG16.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "modelVGG16.fit(train_seq, validation_data=validation_seq, # datos de entrenamiento y validación\n",
    "          epochs=10,\n",
    "          callbacks = [early_stopping, PlotLossesKerasTF()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9332c1-012c-4181-9b7c-089fc6ac5cca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ResNet <a class=\"anchor\" id=\"resnet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf04985-52a9-4de9-a12a-916d851e8b77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(\n",
    "    tf.keras.applications.ResNet101V2(weights=None, include_top=False, input_shape=(*image_size, 3)),\n",
    "    dpi=50,\n",
    "    to_file=\"images/ResNet.jpeg\"\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a86fc-3a87-4c67-a074-fba0cfb80fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelResNet = m.modeloResNet(image_size=image_size)\n",
    "\n",
    "modelResNet.summary()\n",
    "f.memoria_modelo(np.sum(\n",
    "    [np.prod(v.get_shape().as_list()) for v in modelResNet.trainable_variables]\n",
    "), tipo='entrenables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9a685-246d-40c0-823e-9d2ac48bc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preparamos el entrenamiento del modelo --- \n",
    "# [6] https://stackoverflow.com/questions/58565394/what-is-the-difference-between-sparse-categorical-crossentropy-and-categorical-c\n",
    "modelResNet.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Parar el entrenamiento si no hay mejoras significativas (loss)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# --- Entrenamiento ----\n",
    "# Train the model\n",
    "modelResNet.fit(train_seq, validation_data=validation_seq, # datos de entrenamiento y validación\n",
    "          epochs=10,\n",
    "          callbacks = [early_stopping, PlotLossesKerasTF()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b5cf4-2e50-4ab9-9271-302002148b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fine Tuning ---\n",
    "# Liberamos algunas capas para hacer fine tuning\n",
    "capas_resnet_descongeladas = 5\n",
    "for layer in modelResNet.get_layer('resnet101v2').layers[-capas_resnet_descongeladas:]:\n",
    "    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "# Preparamos el entrenamiento del modelo\n",
    "# [6] https://stackoverflow.com/questions/58565394/what-is-the-difference-between-sparse-categorical-crossentropy-and-categorical-c\n",
    "modelResNet.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "modelResNet.fit(train_seq, validation_data=validation_seq, # datos de entrenamiento y validación\n",
    "          epochs=10,\n",
    "          callbacks = [early_stopping, PlotLossesKerasTF()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb07fd72-50f1-403a-8f11-06c73fb58c6a",
   "metadata": {},
   "source": [
    "### Modelos propios  <a class=\"anchor\" id=\"modelos-propios\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b660c99-049e-48ec-810f-46aaf3149585",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Modelo sin regularización Ridge <a class=\"anchor\" id=\"sin-regularizacion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a513de-8252-49bf-8019-507ea20ee079",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = m.modelo_sinRegularizacion(image_size=image_size)\n",
    "\n",
    "model.summary()\n",
    "f.memoria_modelo(np.sum(\n",
    "    [np.prod(v.get_shape().as_list()) for v in model.trainable_variables]\n",
    "), tipo='entrenables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281db3a0-9b1e-43e5-bc28-39ccaccbf21b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d25f8a-2eb9-4a9d-b40c-c40b64d12afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preparamos el entrenamiento del modelo ---\n",
    "# [6] https://stackoverflow.com/questions/58565394/what-is-the-difference-between-sparse-categorical-crossentropy-and-categorical-c\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Parar el entrenamiento si no hay mejoras significativas (loss)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# --- Entrenamiento ----\n",
    "# Train the model\n",
    "model.fit(train_seq, validation_data=validation_seq, # datos de entrenamiento y validación\n",
    "          epochs=30,\n",
    "          callbacks = [early_stopping, PlotLossesKerasTF()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0f81c-0bb7-4eb2-a1fa-aee20060f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fine Tuning ---\n",
    "# Preparamos el entrenamiento del modelo\n",
    "# [6] https://stackoverflow.com/questions/58565394/what-is-the-difference-between-sparse-categorical-crossentropy-and-categorical-c\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_seq, validation_data=validation_seq, # datos de entrenamiento y validación\n",
    "          epochs=30,\n",
    "          callbacks = [early_stopping, PlotLossesKerasTF()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fc7be-a124-4463-baa5-c2e7d2b24c60",
   "metadata": {},
   "source": [
    "Con una tasa de aprendizaje mayor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3f832-39d2-4137-b88d-232ff2bc70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = m.modelo_sinRegularizacion(image_size=image_size)\n",
    "\n",
    "# --- Preparamos el entrenamiento del modelo ---\n",
    "# [6] https://stackoverflow.com/questions/58565394/what-is-the-difference-between-sparse-categorical-crossentropy-and-categorical-c\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(5e-2),\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Parar el entrenamiento si no hay mejoras significativas (loss)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# --- Entrenamiento ----\n",
    "# Train the model\n",
    "model.fit(train_seq, validation_data=validation_seq, # datos de entrenamiento y validación\n",
    "          epochs=30,\n",
    "          callbacks = [early_stopping, PlotLossesKerasTF()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18043f-ee00-44db-bd98-0bc778c51361",
   "metadata": {},
   "source": [
    "#### Modelo con regularización Ridge <a class=\"anchor\" id=\"con-regularizacion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d8012-3afa-4d68-8abf-b33cc268d332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelReg = m.modelo_conRegularizacion(image_size=image_size)\n",
    "\n",
    "modelReg.summary()\n",
    "f.memoria_modelo(np.sum(\n",
    "    [np.prod(v.get_shape().as_list()) for v in modelReg.trainable_variables]\n",
    "), tipo='entrenables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a14ef94-57b3-4ea5-b42e-a8d55f578983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(modelReg, to_file=\"images/mi_modelo.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1490d-6755-4ca8-a7dd-be04f93b0a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preparamos el entrenamiento del modelo ---\n",
    "# [6] https://stackoverflow.com/questions/58565394/what-is-the-difference-between-sparse-categorical-crossentropy-and-categorical-c\n",
    "modelReg.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Parar el entrenamiento si no hay mejoras significativas (loss)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# --- Entrenamiento ----\n",
    "# Train the model\n",
    "modelReg.fit(train_seq, validation_data=validation_seq, # datos de entrenamiento y validación\n",
    "          epochs=30,\n",
    "          callbacks = [early_stopping, PlotLossesKerasTF()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85331848-be55-460c-87eb-60ec46c2aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fine Tuning ---\n",
    "# Preparamos el entrenamiento del modelo\n",
    "# [6] https://stackoverflow.com/questions/58565394/what-is-the-difference-between-sparse-categorical-crossentropy-and-categorical-c\n",
    "modelReg.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "modelReg.fit(train_seq, validation_data=validation_seq, # datos de entrenamiento y validación\n",
    "          epochs=30,\n",
    "          callbacks = [early_stopping, PlotLossesKerasTF()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee8dbec-f352-453d-a5bb-bc80fc6c2767",
   "metadata": {},
   "source": [
    "# Resultados <a class=\"anchor\" id=\"resultados\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88acf32a-32d4-42ee-b7ae-10c791a7be0b",
   "metadata": {},
   "source": [
    "## Modelo propio con regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100c5b4-0cf8-4d5d-be27-0a697e3762b1",
   "metadata": {},
   "source": [
    "### Precisión, recall, F1-score y confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382866ef-0b37-40f5-ab0c-4a055c094268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [13] https://www.kaggle.com/code/fathyalin/rock-paper-scissors-tensorflow-with-real-photos/notebook\n",
    "\n",
    "predictions = np.argmax(modelReg.predict(test_seq), axis=1)\n",
    "reales = [test_seq.class_mapping[test_seq.labels[path]] for path in test_seq.image_paths]\n",
    "\n",
    "acc = modelReg.evaluate(test_seq, verbose=0)[1]\n",
    "otras_medidas = pd.DataFrame(skm.precision_recall_fscore_support(reales, predictions), \n",
    "                             columns=(\"Piedra\", \"Papel\", \"Tijeras\"), \n",
    "                             index=[\"Precision\", \"Recall\", \"F1-score\", \"Número de casos\"]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3fd93-8cd3-4466-bfbf-185a55b13f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=reales, y_pred=predictions, labels=[0,1,2])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm, annot=True, fmt='g', vmin=0, cbar=False)\n",
    "plt.xticks(ticks=[0.5, 1.5, 2.5], labels=['Piedra','Papel','Tijeras'])\n",
    "plt.yticks(ticks=[0.5, 1.5, 2.5], labels=['Piedra','Papel','Tijeras'])\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de confusión\")\n",
    "plt.savefig('images/propia/confusion_matrix.jpeg', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Medidas de rendimiento\\n', \n",
    "      f'Accuracy (sobre test): {np.round(acc*100, 1)}%\\n',\n",
    "     otras_medidas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf9aa1-61ae-4819-9b19-14524c59ee0a",
   "metadata": {},
   "source": [
    "### Clasificación de imágenes en correctas e incorrectas\n",
    "\n",
    "La clasificación en una de las categorías depende de si esta correcta o incorrectamente clasificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c284c535-5467-434b-9ddd-4878804cff14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pre-cargamos las lista de índices de imágenes correcta e incorrectamente clasificadas\n",
    "lista_idx_correctos = []\n",
    "lista_idx_incorrectos = []\n",
    "\n",
    "# Cargamos batches de 1 imagen para poder usarlas todas\n",
    "# Si tenemos 307 imágenes y tomamos batches de 100 se pierden las 7 últimas\n",
    "test_seq.batch_size = 1\n",
    "n_imgs = len(test_seq)\n",
    "\n",
    "for idx in range(n_imgs):\n",
    "    # Mostrar el progreso\n",
    "    print('\\r', f'{idx+1} / {n_imgs}', end='')\n",
    "    \n",
    "    # Cargamos la imágen, su clase real y la predicha\n",
    "    im, clase_real = test_seq[idx]\n",
    "    im = np.copy(im)\n",
    "    clase_pred = np.argmax(modelReg.predict(im, verbose=0), axis=1)[0]\n",
    "\n",
    "    # Comprobamos si la imágen se clasifica correctamente o no\n",
    "    if clase_real == clase_pred:\n",
    "        lista_idx_correctos.append(idx)\n",
    "    else:\n",
    "        lista_idx_incorrectos.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d40f6d-8fbb-4a21-a92c-0a3e0fe252da",
   "metadata": {},
   "source": [
    "### Casos incorrectamente clasificados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56528ec9-a1f7-48a2-8114-1b83251d397b",
   "metadata": {},
   "source": [
    "#### Análisis de predicciones incorrectas\n",
    "Práctica 6 (Grad-CAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c43ebd9-3329-4bd2-a5a0-5e1b40803d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bda0a-7183-48a5-952f-da94824a0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Índices de imágenes incorrectas desordenado\n",
    "np.random.shuffle(lista_idx_incorrectos)\n",
    "\n",
    "# Plot\n",
    "images = f.imagenes_test(modelReg, test_seq, lista_idx_incorrectos)\n",
    "f.show_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98eeaaa-9798-47fb-8703-329e9c9ae23e",
   "metadata": {},
   "source": [
    "#### Heatmaps de casos incorrectamente clasificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c1462-6440-46a0-b06c-6634fdeef8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lista_idx_incorrectos:\n",
    "    f.show_heatmaps(images, modelReg, clasificacion='propia/clasificacion_incorrecta')\n",
    "else:\n",
    "    print(\"No hay imágenes incorrectamente clasificadas!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e4b14-9122-4eeb-8662-a9496447bf72",
   "metadata": {},
   "source": [
    "### Casos correctamente clasificados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fab25e-04e7-43f9-af93-5019a0997785",
   "metadata": {},
   "source": [
    "#### Análisis de predicciones incorrectas\n",
    "Práctica 6 (Grad-CAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43935cfd-97b7-4bb5-be43-03526c0d4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Índices de imágenes incorrectas desordenado\n",
    "np.random.shuffle(lista_idx_correctos)\n",
    "\n",
    "# Plot\n",
    "images = f.imagenes_test(modelReg, test_seq, lista_idx_correctos)\n",
    "f.show_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488e021-c04c-4b74-acf6-1fa95be149b6",
   "metadata": {},
   "source": [
    "#### Heatmaps de casos correctamente clasificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bff3fa-27ac-4ef2-9ae8-912f541444ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.show_heatmaps(images, modelReg, clasificacion='propia/clasificacion_correcta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190e973-35d2-4487-89a9-d0f6c499befe",
   "metadata": {},
   "source": [
    "### Visualización de los filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869e383-179a-4071-8f49-d0f32d37f3a5",
   "metadata": {},
   "source": [
    "#### Visualización de las activaciones de los filtros convolucionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de6851-547e-44d3-afca-e58e714ae07c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(f)\n",
    "\n",
    "f.visualizacion_filtros(modelReg, n_rows_grid=8, clasificacion='propia')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e23e46-ad03-4aa1-b165-2494935ef8a6",
   "metadata": {},
   "source": [
    "#### Kernels y mapas de características de la primera capa convolucional sobre una imagen correctamente clasificada\n",
    "\n",
    "https://www.kaggle.com/code/sanjitschouhan/visualizing-conv2d-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af8fbf-8bd1-4c6c-9750-18e5ef47e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(33)\n",
    "\n",
    "# Seleccionamos una imagen (de las bien clasificadas) aleatoriamente\n",
    "np.random.shuffle(lista_idx_correctos)\n",
    "idx = lista_idx_correctos[0]\n",
    "im, clase_real = test_seq[idx]\n",
    "im = np.copy(im)\n",
    "\n",
    "# Plot\n",
    "# [14] https://stackoverflow.com/questions/50630825/matplotlib-imshow-distorting-colors\n",
    "plt.imshow(im[0][...,::-1])\n",
    "plt.title(f\"Clase real: {('Piedra', 'Papel', 'Tijeras')[clase_real[0]]}\")\n",
    "plt.savefig('images/propia/ejemplo_imagen_correcta.jpeg', \n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b3377-e868-4853-9cfd-736d8c3762dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtener los kernels de la primera capa convolucional\n",
    "primera_conv2d = modelReg.get_layer('conv2d_entrada')\n",
    "kernels, _ = primera_conv2d.get_weights()\n",
    "n_kernels = kernels.shape[-1]\n",
    "\n",
    "plt.figure(figsize=(20,100))\n",
    "\n",
    "# Número de filas a dibujar\n",
    "n_filters = kernels.shape[-1]\n",
    "n_rows = n_filters // 3 + 1\n",
    "\n",
    "for idx in range(n_kernels):\n",
    "  # Obtener el kernel\n",
    "  kernel = np.array(kernels[:,:,:,idx])\n",
    "  kernel_in = tf.constant(\n",
    "      np.reshape(kernel, (*kernel.shape, 1))\n",
    "  )\n",
    "\n",
    "  # Imagen\n",
    "  image_in = tf.constant(\n",
    "      np.reshape(im, (1,128,128,3)), dtype=tf.float32\n",
    "      )\n",
    "\n",
    "  # Aplicamos el kernel a la imagen correctamente clasificada\n",
    "  salida = tf.nn.conv2d(image_in,\n",
    "                        filters = kernel_in,\n",
    "                        strides = [1, 1, 1, 1],\n",
    "                        padding = 'SAME')\n",
    "    \n",
    "  # Plot\n",
    "  # [15] https://stackoverflow.com/questions/49643907/clipping-input-data-to-the-valid-range-for-imshow-with-rgb-data-0-1-for-floa\n",
    "  plt.subplot(n_rows,6, 2*idx+1)\n",
    "  plt.imshow((kernel * 255).astype(np.uint8))\n",
    "  plt.title(\"Filter Kernel \"+str(idx+1))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.subplot(n_rows,6, 2*idx+2)\n",
    "  plt.imshow(np.reshape(salida * 255, (128,128)).astype(np.uint8), cmap='gray')\n",
    "\n",
    "  plt.title(\"Kernel Output \"+str(idx+1))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "plt.savefig('images/propia/activaciones/kernels_primera_convolucional.jpeg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319899d-a10a-4c76-9f68-2f63ce11dbac",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a177a5-9b8c-4d2e-81fc-b4d8dcaba750",
   "metadata": {},
   "source": [
    "### Precisión, recall, F1-score y confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e49f4b-a7d6-4fa3-87ac-7c747857e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [13] https://www.kaggle.com/code/fathyalin/rock-paper-scissors-tensorflow-with-real-photos/notebook\n",
    "\n",
    "predictions = np.argmax(modelVGG16.predict(test_seq), axis=1)\n",
    "reales = [test_seq.class_mapping[test_seq.labels[path]] for path in test_seq.image_paths]\n",
    "\n",
    "acc = modelVGG16.evaluate(test_seq, verbose=0)[1]\n",
    "otras_medidas = pd.DataFrame(skm.precision_recall_fscore_support(reales, predictions), \n",
    "                             columns=(\"Piedra\", \"Papel\", \"Tijeras\"), \n",
    "                             index=[\"Precision\", \"Recall\", \"F1-score\", \"Número de casos\"]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233ad0f-1587-4e2b-8a8e-0f54205814c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=reales, y_pred=predictions, labels=[0,1,2])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm, annot=True, fmt='g', vmin=0, cbar=False)\n",
    "plt.xticks(ticks=[0.5, 1.5, 2.5], labels=['Piedra','Papel','Tijeras'])\n",
    "plt.yticks(ticks=[0.5, 1.5, 2.5], labels=['Piedra','Papel','Tijeras'])\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de confusión\")\n",
    "plt.savefig('images/vgg16/confusion_matrix.jpeg', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Medidas de rendimiento\\n', \n",
    "      f'Accuracy (sobre test): {np.round(acc*100, 2)}%\\n',\n",
    "     otras_medidas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d4c6fa-5dee-4a82-9c13-cd97135bf57f",
   "metadata": {},
   "source": [
    "### Clasificación de imágenes en correctas e incorrectas\n",
    "\n",
    "La clasificación en una de las categorías depende de si esta correcta o incorrectamente clasificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a0175-2733-47cb-9dbe-f8b667003dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pre-cargamos las lista de índices de imágenes correcta e incorrectamente clasificadas\n",
    "lista_idx_correctos = []\n",
    "lista_idx_incorrectos = []\n",
    "\n",
    "# Cargamos batches de 1 imagen para poder usarlas todas\n",
    "# Si tenemos 307 imágenes y tomamos batches de 100 se pierden las 7 últimas\n",
    "test_seq.batch_size = 1\n",
    "n_imgs = len(test_seq)\n",
    "\n",
    "for idx in range(n_imgs):\n",
    "    # Mostrar el progreso\n",
    "    print('\\r', f'{idx+1} / {n_imgs}', end='')\n",
    "    \n",
    "    # Cargamos la imágen, su clase real y la predicha\n",
    "    im, clase_real = test_seq[idx]\n",
    "    im = np.copy(im)\n",
    "    clase_pred = np.argmax(modelVGG16.predict(im, verbose=0), axis=1)[0]\n",
    "\n",
    "    # Comprobamos si la imágen se clasifica correctamente o no\n",
    "    if clase_real == clase_pred:\n",
    "        lista_idx_correctos.append(idx)\n",
    "    else:\n",
    "        lista_idx_incorrectos.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c2d1d-1e75-4968-b2e1-f987c9030c60",
   "metadata": {},
   "source": [
    "### Casos incorrectamente clasificados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdbceb2-e14a-47aa-a51f-b55dafe89d53",
   "metadata": {},
   "source": [
    "#### Análisis de predicciones incorrectas\n",
    "Práctica 6 (Grad-CAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f432f3-2468-4fab-b1f3-8178ada778dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da128844-a8e8-4f13-b8b0-5aa9c2a9af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Índices de imágenes incorrectas desordenado\n",
    "np.random.shuffle(lista_idx_incorrectos)\n",
    "\n",
    "# Plot\n",
    "images = f.imagenes_test(modelVGG16, test_seq, lista_idx_incorrectos)\n",
    "f.show_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186f168-74c8-437d-a0f7-00538397ae22",
   "metadata": {},
   "source": [
    "#### Heatmaps de casos incorrectamente clasificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fd0ab-c3ae-4aa7-aa22-eb2e23217e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3299523d-5b0c-4902-8eea-1ee41ac15d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lista_idx_incorrectos:\n",
    "    f.show_heatmaps(images, modelVGG16.get_layer('vgg16'), n_finales=capas_vgg_descongeladas, clasificacion='vgg16/clasificacion_incorrecta')\n",
    "else:\n",
    "    print(\"No hay imágenes incorrectamente clasificadas!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e275b15-9a4c-44e4-83a8-4af146144e3e",
   "metadata": {},
   "source": [
    "### Casos correctamente clasificados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63efac77-e15e-4f9e-ad7e-a5ffcf9d78da",
   "metadata": {},
   "source": [
    "#### Análisis de predicciones incorrectas\n",
    "Práctica 6 (Grad-CAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff425ee-edb6-4682-848a-6c6f66848fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Índices de imágenes incorrectas desordenado\n",
    "np.random.shuffle(lista_idx_correctos)\n",
    "\n",
    "# Plot\n",
    "images = f.imagenes_test(modelVGG16, test_seq, lista_idx_correctos)\n",
    "f.show_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe57625-067c-43d9-a87a-17213179047d",
   "metadata": {},
   "source": [
    "#### Heatmaps de casos correctamente clasificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73026e36-bd9d-46cc-bbdc-33a006aeee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.show_heatmaps(images, modelVGG16.get_layer('vgg16'), n_finales=0, clasificacion='vgg16/clasificacion_correcta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed081b9-878a-44ef-87ce-68599c9bd0d1",
   "metadata": {},
   "source": [
    "### Visualización de los filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109b5cf-1a6a-4ba4-a014-ebb790c5387d",
   "metadata": {},
   "source": [
    "#### Visualización de las activaciones de los filtros convolucionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb950d-a546-428b-b21a-2f72f67093fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(f)\n",
    "\n",
    "f.visualizacion_filtros(modelVGG16.get_layer('vgg16'), n_rows_grid=8, n_top_layers=capas_vgg_descongeladas, clasificacion='vgg16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1725d74f-a918-4922-80d5-6c45b9592b13",
   "metadata": {},
   "source": [
    "#### Kernels y mapas de características de la primera capa convolucional sobre una imagen correctamente clasificada\n",
    "\n",
    "https://www.kaggle.com/code/sanjitschouhan/visualizing-conv2d-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f217506-45bf-431a-965b-b39edd34e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(33)\n",
    "\n",
    "# Seleccionamos una imagen (de las bien clasificadas) aleatoriamente\n",
    "np.random.shuffle(lista_idx_correctos)\n",
    "idx = lista_idx_correctos[0]\n",
    "im, clase_real = test_seq[idx]\n",
    "im = np.copy(im)\n",
    "\n",
    "# Plot\n",
    "# [14] https://stackoverflow.com/questions/50630825/matplotlib-imshow-distorting-colors\n",
    "plt.imshow(im[0][...,::-1])\n",
    "plt.title(f\"Clase real: {('Piedra', 'Papel', 'Tijeras')[clase_real[0]]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca339b6-04e9-4b86-a59f-c7ed39832012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtener los kernels de la primera capa convolucional\n",
    "primera_conv2d = modelVGG16.get_layer('vgg16').get_layer('block1_conv1')\n",
    "kernels, _ = primera_conv2d.get_weights()\n",
    "n_kernels = kernels.shape[-1]\n",
    "\n",
    "plt.figure(figsize=(20,100))\n",
    "\n",
    "# Número de filas a dibujar\n",
    "n_filters = kernels.shape[-1]\n",
    "n_rows = n_filters // 3 + 1\n",
    "\n",
    "for idx in range(n_kernels):\n",
    "  # Obtener el kernel\n",
    "  kernel = np.array(kernels[:,:,:,idx])\n",
    "  kernel_in = tf.constant(\n",
    "      np.reshape(kernel, (*kernel.shape, 1))\n",
    "  )\n",
    "\n",
    "  # Imagen\n",
    "  image_in = tf.constant(\n",
    "      np.reshape(im, (1,128,128,3)), dtype=tf.float32\n",
    "      )\n",
    "\n",
    "  # Aplicamos el kernel a la imagen correctamente clasificada\n",
    "  salida = tf.nn.conv2d(image_in,\n",
    "                        filters = kernel_in,\n",
    "                        strides = [1, 1, 1, 1],\n",
    "                        padding = 'SAME')\n",
    "    \n",
    "  # Plot\n",
    "  # [15] https://stackoverflow.com/questions/49643907/clipping-input-data-to-the-valid-range-for-imshow-with-rgb-data-0-1-for-floa\n",
    "  plt.subplot(n_rows,6, 2*idx+1)\n",
    "  plt.imshow((kernel * 255).astype(np.uint8))\n",
    "  plt.title(\"Filter Kernel \"+str(idx+1))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.subplot(n_rows,6, 2*idx+2)\n",
    "  plt.imshow(np.reshape(salida * 255, (128,128)).astype(np.uint8), cmap='gray')\n",
    "\n",
    "  plt.title(\"Kernel Output \"+str(idx+1))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "plt.savefig('images/vgg16/activaciones/kernels_primera_convolucional.jpeg', \n",
    "            bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
